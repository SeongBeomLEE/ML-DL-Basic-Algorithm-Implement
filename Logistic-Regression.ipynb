{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BCE Loss를 이용한 Logistic-Regression (이진 분류) \n",
    "\n",
    "- BCE를 손실함수로 사용하고 경사하강법을 이용하여 모델의 가중치를 업데이트\n",
    "\n",
    "- 경사하강법은 SGD가 아닌 전체 데이터를 사용하는 Batch GD를 사용함\n",
    "\n",
    "- Y_hat = sigmoid(WX + b) * threshold = 0.5\n",
    "\n",
    "- Loss = BCE = -(Y * log(Y_hat) + ((1 - Y) * log(1 - Y_hat))) / N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def sigmoid(X):\n",
    "    return 1 / (1 + np.exp(-X))\n",
    "\n",
    "threshold = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 생성\n",
    "\n",
    "X = np.random.randn(1000, 10)\n",
    "W = np.random.rand(10)\n",
    "b = np.random.rand()\n",
    "\n",
    "Y = X.dot(W) + b\n",
    "Y = sigmoid(Y)\n",
    "Y = (Y >= threshold).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegressionBCE:\n",
    "    def __init__(self, input_dim, lr, epochs, seed):\n",
    "        self.input_dim = input_dim\n",
    "        self.lr = lr\n",
    "        self.epochs = epochs\n",
    "        np.random.seed(seed)\n",
    "        self.W = np.random.rand(input_dim)\n",
    "        self.b = 0.0\n",
    "    \n",
    "    def backword(self, X, Y, Y_hat):\n",
    "        dE = (Y_hat - Y).reshape(-1, 1) # (data, 1)\n",
    "        dW = np.mean(dE * X, axis = 0) # (data, input_dim) -> (1, input_dim)\n",
    "        db = np.mean(dE) # (data, 1) -> (1,)\n",
    "\n",
    "        self.W -= dW * self.lr\n",
    "        self.b -= db * self.lr\n",
    "\n",
    "    def forword(self, X, threshold = 0.5):\n",
    "        Y_hat = sigmoid(X.dot(self.W) + self.b)\n",
    "        Y_hat = (Y_hat >= threshold).astype(int)\n",
    "        return Y_hat\n",
    "\n",
    "    def loss(self, Y, Y_hat):\n",
    "        epsilon = 1e-10\n",
    "        return -np.mean(Y * np.log(Y_hat + epsilon) + (1 - Y) * np.log((1 - Y_hat) + epsilon))\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        '''\n",
    "        X = (data, input_dim)\n",
    "        Y = (data, 1)\n",
    "        '''\n",
    "\n",
    "        for epoch in range(1, self.epochs + 1):\n",
    "            Y_hat = self.forword(X)\n",
    "            error = self.loss(Y, Y_hat)\n",
    "            if epoch % 1000 == 0:\n",
    "                print(f'Epoch: {epoch} | Loss : {error} | Acc : {sum(Y_hat == Y) / len(Y)}')\n",
    "            self.backword(X, Y, Y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegressionBCE(input_dim = X.shape[1], lr = 0.01, epochs = 5000, seed = 22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1000 | Loss : 0.34538776385060704 | Acc : 0.985\n",
      "Epoch: 2000 | Loss : 0.04605170176008093 | Acc : 0.998\n",
      "Epoch: 3000 | Loss : 0.06907755269012142 | Acc : 0.997\n",
      "Epoch: 4000 | Loss : -1.0000000826903712e-10 | Acc : 1.0\n",
      "Epoch: 5000 | Loss : -1.0000000826903712e-10 | Acc : 1.0\n"
     ]
    }
   ],
   "source": [
    "model.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score:1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression(penalty=\"none\")\n",
    "model.fit(X, Y)\n",
    "print(f\"Score:{model.score(X, Y)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습 결과 target과 가까워지게 W와 b가 업데이트 되지는 않지만(sigmoid를 취한 아웃풋에 임계치를 기반으로 예측을 하기 때문에, 정확한 회귀선을 추정하지 않더라도 값을 표현하는 모델을 만들 수 있음), ACC가 올라간다는 것을 확인할 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CE Loss를 이용한 Logistic-Regression (2개 이상의 class 분류) \n",
    "\n",
    "- CE를 손실함수로 사용하고 경사하강법을 이용하여 모델의 가중치를 업데이트\n",
    "\n",
    "- 경사하강법은 SGD가 아닌 전체 데이터를 사용하는 Batch GD를 사용함\n",
    "\n",
    "- Y_hat = argmax(softmax(WX + b))\n",
    "\n",
    "- Loss = CE = -(Y * log(Y_hat)) / N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def softmax(X):\n",
    "    e_x = np.exp(X)\n",
    "    return e_x / np.sum(e_x, 1).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 생성\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "data = load_iris()\n",
    "\n",
    "X = data.data\n",
    "Y = data.target\n",
    "classse = len(data.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegressionCE:\n",
    "    def __init__(self, input_dim, classse, lr, epochs, seed):\n",
    "        self.input_dim = input_dim\n",
    "        self.classse = classse\n",
    "        self.lr = lr\n",
    "        self.epochs = epochs\n",
    "        np.random.seed(seed)\n",
    "        self.W = np.random.rand(input_dim, classse)\n",
    "        self.b = np.zeros(classse)\n",
    "    \n",
    "    def backword(self, X, Y, Y_hat):\n",
    "        dE = (Y_hat - Y) # (data, classse)\n",
    "        dW = np.dot(X.T, dE) # (input_dim, data).dot(data, classse) -> (input_dim, classse)\n",
    "        db = np.mean(dE, axis = 0) # (data, classse) -> (classse,)\n",
    "\n",
    "        self.W -= dW * self.lr\n",
    "        self.b -= db * self.lr\n",
    "\n",
    "    def forword(self, X):\n",
    "        Y_hat = softmax(X.dot(self.W) + self.b)\n",
    "        return Y_hat\n",
    "\n",
    "    def loss(self, Y, Y_hat):\n",
    "        epsilon = 1e-10\n",
    "        return -np.mean(Y * np.log(Y_hat + epsilon))\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        '''\n",
    "        X = (data, input_dim)\n",
    "        Y = (data, 1)\n",
    "        '''\n",
    "        Y = np.eye(self.classse)[Y]\n",
    "        \n",
    "        for epoch in range(1, self.epochs + 1):\n",
    "            Y_hat = self.forword(X)\n",
    "            error = self.loss(Y, Y_hat)\n",
    "            if epoch % 100 == 0:\n",
    "                print(f'Epoch: {epoch} | Loss : {error} | Acc : {sum(np.argmax(Y_hat, axis=1) == np.argmax(Y, axis=1)) / len(Y)}')\n",
    "            self.backword(X, Y, Y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegressionCE(input_dim = X.shape[1], classse = classse, lr = 0.0005, epochs = 1000, seed = 22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100 | Loss : 0.129196993813102 | Acc : 0.9533333333333334\n",
      "Epoch: 200 | Loss : 0.101698627436187 | Acc : 0.9666666666666667\n",
      "Epoch: 300 | Loss : 0.08614956566334889 | Acc : 0.9666666666666667\n",
      "Epoch: 400 | Loss : 0.07588508576471162 | Acc : 0.9733333333333334\n",
      "Epoch: 500 | Loss : 0.06857670197925987 | Acc : 0.9733333333333334\n",
      "Epoch: 600 | Loss : 0.06310056341759517 | Acc : 0.9733333333333334\n",
      "Epoch: 700 | Loss : 0.05883938113601851 | Acc : 0.9733333333333334\n",
      "Epoch: 800 | Loss : 0.05542539628704162 | Acc : 0.9733333333333334\n",
      "Epoch: 900 | Loss : 0.05262591979334622 | Acc : 0.9733333333333334\n",
      "Epoch: 1000 | Loss : 0.05028657214104328 | Acc : 0.9733333333333334\n"
     ]
    }
   ],
   "source": [
    "model.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score:0.9866666666666667\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression(penalty=\"none\")\n",
    "model.fit(X, Y)\n",
    "print(f\"Score:{model.score(X, Y)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5836ffa1101046057468fdfe98fbd6981fde0b239b5104b10a8f4b084de20616"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
